{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c9626ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /usr/local/lib/python3.7/site-packages (2.8.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8bca4d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43b1286b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>normalized_text_stemm_emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-07-04 22:17:21</td>\n",
       "      <td>RT @emirsader: Após ficar em silêncio na CPI d...</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>[após, fic, silênci, cpi, carl, wizard, retorn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-11-30 14:35:33</td>\n",
       "      <td>RT @VittorGuidoni: para de lamber fone de ouvi...</td>\n",
       "      <td>Negativo</td>\n",
       "      <td>[lamb, fon, ouv, list, doenç, transmiss, oral,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-11 12:12:28</td>\n",
       "      <td>@exposed_exposer @CarlaZambelli38 @andrizek De...</td>\n",
       "      <td>Negativo</td>\n",
       "      <td>[defin, vacin, tip, subst, bactér, introduz, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-11-19 14:52:06</td>\n",
       "      <td>RT @canaltech: Por que o iPhone é mais seguro ...</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>[iphon, segur, contr, ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-12-28 23:09:10</td>\n",
       "      <td>RT @mariareinhardtt: covid tá a estragar compl...</td>\n",
       "      <td>Negativo</td>\n",
       "      <td>[estrag, complet, tud, ]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            created_at                                               text  \\\n",
       "0  2021-07-04 22:17:21  RT @emirsader: Após ficar em silêncio na CPI d...   \n",
       "1  2019-11-30 14:35:33  RT @VittorGuidoni: para de lamber fone de ouvi...   \n",
       "2  2022-01-11 12:12:28  @exposed_exposer @CarlaZambelli38 @andrizek De...   \n",
       "3  2019-11-19 14:52:06  RT @canaltech: Por que o iPhone é mais seguro ...   \n",
       "4  2021-12-28 23:09:10  RT @mariareinhardtt: covid tá a estragar compl...   \n",
       "\n",
       "  sentiment                        normalized_text_stemm_emoji  \n",
       "0  Positivo  [após, fic, silênci, cpi, carl, wizard, retorn...  \n",
       "1  Negativo  [lamb, fon, ouv, list, doenç, transmiss, oral,...  \n",
       "2  Negativo  [defin, vacin, tip, subst, bactér, introduz, c...  \n",
       "3  Positivo                            [iphon, segur, contr, ]  \n",
       "4  Negativo                           [estrag, complet, tud, ]  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data_pt_brazil_normalized_emoji_merged.csv',\n",
    "                                converters={\n",
    "                     \"normalized_text_stemm_emoji\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")\n",
    "                 })\n",
    "df = df[['created_at', 'text', 'sentiment', 'normalized_text_stemm_emoji']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b253e66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_bank = {\n",
    "  \",:(\": \"😓\",\n",
    "  \",:)\": \"😅\",\n",
    "  \",:-(\": \"😓\",\n",
    "  \",:-)\": \"😅\",\n",
    "  \"0:)\": \"😇\",\n",
    "  \"0:-)\": \"😇\",\n",
    "  \"8-)\": \"😎\",\n",
    "  \":$\": \"😒\",\n",
    "  \":'(\": \"😢\",\n",
    "  \":')\": \"😂\",\n",
    "  \":'-(\": \"😢\",\n",
    "  \":'-)\": \"😂\",\n",
    "  \":'-D\": \"😂\",\n",
    "  \":'D\": \"😂\",\n",
    "  \":(\": \"😦\",\n",
    "  \":)\": \"😃\",\n",
    "  \":*\": \"😗\",\n",
    "  \":,'(\": \"😭\",\n",
    "  \":,'-(\": \"😭\",\n",
    "  \":,(\": \"😢\",\n",
    "  \":,)\": \"😂\",\n",
    "  \":,-(\": \"😢\",\n",
    "  \":,-)\": \"😂\",\n",
    "  \":,-D\": \"😂\",\n",
    "  \":,D\": \"😂\",\n",
    "  \":-$\": \"😒\",\n",
    "  \":-(\": \"😦\",\n",
    "  \":-)\": \"😃\",\n",
    "  \":-*\": \"😗\",\n",
    "  \":-/\": \"😕\",\n",
    "  \":-@\": \"😡\",\n",
    "  \":-D\": \"😄\",\n",
    "  \":-o\": \"😮\",\n",
    "  \":-O\": \"😮\",\n",
    "  \":-P\": \"😛\",\n",
    "  \":-S\": \"😒\",\n",
    "  \":-Z\": \"😒\",\n",
    "  \":-|\": \"😐\",\n",
    "  \":/\": \"😕\",\n",
    "  \":@\": \"😡\",\n",
    "  \":D\": \"😄\",\n",
    "  \":o\": \"😮\",\n",
    "  \":O\": \"😮\",\n",
    "  \":P\": \"😛\",\n",
    "  \":s\": \"😒\",\n",
    "  \":z\": \"😒\",\n",
    "  \":|\": \"😐\",\n",
    "  \";(\": \"😭\",\n",
    "  \";)\": \"😉\",\n",
    "  \";-(\": \"😭\",\n",
    "  \";-)\": \"😉\",\n",
    "  \"]:)\": \"😈\",\n",
    "  \"]:-)\": \"😈\",\n",
    "  \"B-)\": \"😎\",\n",
    "  \"o:)\": \"😇\",\n",
    "  \"O:)\": \"😇\",\n",
    "  \"O:-)\": \"😇\",\n",
    "  \"o:-)\": \"😇\",\n",
    "  \"X-)\": \"😆\",\n",
    "  \"x-)\": \"😆\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26621668",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = ['rt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9bcfd786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import emoji\n",
    "\n",
    "def extract_emojis(sample):\n",
    "    return ' '.join(c for c in sample if c in emoji.EMOJI_DATA).split()\n",
    "\n",
    "def remove_user_from_text(words):\n",
    "    return \" \".join(filter(lambda x:x[0]!='@', words.split()))\n",
    "\n",
    "def remove_double_space(sample):\n",
    "    return \" \".join(sample.split())\n",
    "\n",
    "def remove_URL(sample):\n",
    "    \"\"\"Remove URLs from a sample string\"\"\"\n",
    "    return re.sub(r\"http\\S+\", \"\", sample)\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    return list(map(lambda x: x.lower(), words))\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def clean_text(sample):\n",
    "    emoji_pat = '[\\U0001F300-\\U0001F64F\\U0001F680-\\U0001F6FF\\u2600-\\u26FF\\u2700-\\u27BF]'\n",
    "    shrink_whitespace_reg = re.compile(r'\\s{2,}')\n",
    "    reg = re.compile(r'({})|[^a-zA-Z]'.format(emoji_pat)) # line a\n",
    "    result = reg.sub(lambda x: ' {} '.format(x.group(1)) if x.group(1) else ' ', sample)\n",
    "    return shrink_whitespace_reg.sub(' ', result)\n",
    "\n",
    "def replace_asci_emoji(sample):\n",
    "    address = sample\n",
    "    for k,v in emoji_bank.items():\n",
    "        address = address.replace(k, v)\n",
    "    return address\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords_list:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_user_from_text(words)\n",
    "    words = to_lowercase(words.split())\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "def preprocess(sample):\n",
    "    sample = remove_URL(sample)\n",
    "    sample = replace_asci_emoji(sample)\n",
    "#     sample = clean_text(sample)\n",
    "    sample = remove_double_space(sample)\n",
    "    return normalize(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "379b391a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>normalized_text_stemm_emoji</th>\n",
       "      <th>normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-07-04 22:17:21</td>\n",
       "      <td>RT @emirsader: Após ficar em silêncio na CPI d...</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>[após, fic, silênci, cpi, carl, wizard, retorn...</td>\n",
       "      <td>após ficar em silêncio na cpi da covid carlos ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-11-30 14:35:33</td>\n",
       "      <td>RT @VittorGuidoni: para de lamber fone de ouvi...</td>\n",
       "      <td>Negativo</td>\n",
       "      <td>[lamb, fon, ouv, list, doenç, transmiss, oral,...</td>\n",
       "      <td>para de lamber fone de ouvido lista de doenças...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-11 12:12:28</td>\n",
       "      <td>@exposed_exposer @CarlaZambelli38 @andrizek De...</td>\n",
       "      <td>Negativo</td>\n",
       "      <td>[defin, vacin, tip, subst, bactér, introduz, c...</td>\n",
       "      <td>definição de vacina é um tipo de substância ví...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-11-19 14:52:06</td>\n",
       "      <td>RT @canaltech: Por que o iPhone é mais seguro ...</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>[iphon, segur, contr, ]</td>\n",
       "      <td>por que o iphone é mais seguro contra vírus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-12-28 23:09:10</td>\n",
       "      <td>RT @mariareinhardtt: covid tá a estragar compl...</td>\n",
       "      <td>Negativo</td>\n",
       "      <td>[estrag, complet, tud, ]</td>\n",
       "      <td>covid tá a estragar completamente tudo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            created_at                                               text  \\\n",
       "0  2021-07-04 22:17:21  RT @emirsader: Após ficar em silêncio na CPI d...   \n",
       "1  2019-11-30 14:35:33  RT @VittorGuidoni: para de lamber fone de ouvi...   \n",
       "2  2022-01-11 12:12:28  @exposed_exposer @CarlaZambelli38 @andrizek De...   \n",
       "3  2019-11-19 14:52:06  RT @canaltech: Por que o iPhone é mais seguro ...   \n",
       "4  2021-12-28 23:09:10  RT @mariareinhardtt: covid tá a estragar compl...   \n",
       "\n",
       "  sentiment                        normalized_text_stemm_emoji  \\\n",
       "0  Positivo  [após, fic, silênci, cpi, carl, wizard, retorn...   \n",
       "1  Negativo  [lamb, fon, ouv, list, doenç, transmiss, oral,...   \n",
       "2  Negativo  [defin, vacin, tip, subst, bactér, introduz, c...   \n",
       "3  Positivo                            [iphon, segur, contr, ]   \n",
       "4  Negativo                           [estrag, complet, tud, ]   \n",
       "\n",
       "                                          normalized  \n",
       "0  após ficar em silêncio na cpi da covid carlos ...  \n",
       "1  para de lamber fone de ouvido lista de doenças...  \n",
       "2  definição de vacina é um tipo de substância ví...  \n",
       "3        por que o iphone é mais seguro contra vírus  \n",
       "4             covid tá a estragar completamente tudo  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['normalized'] = df.apply(lambda x: \" \".join(preprocess(x['text'])), axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f9a926b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>normalized</th>\n",
       "      <th>normalized_text_stemm_emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-07-04 22:17:21</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>após ficar em silêncio na cpi da covid carlos ...</td>\n",
       "      <td>[após, fic, silênci, cpi, carl, wizard, retorn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-11-30 14:35:33</td>\n",
       "      <td>Negativo</td>\n",
       "      <td>para de lamber fone de ouvido lista de doenças...</td>\n",
       "      <td>[lamb, fon, ouv, list, doenç, transmiss, oral,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-11 12:12:28</td>\n",
       "      <td>Negativo</td>\n",
       "      <td>definição de vacina é um tipo de substância ví...</td>\n",
       "      <td>[defin, vacin, tip, subst, bactér, introduz, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-11-19 14:52:06</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>por que o iphone é mais seguro contra vírus</td>\n",
       "      <td>[iphon, segur, contr, ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-12-28 23:09:10</td>\n",
       "      <td>Negativo</td>\n",
       "      <td>covid tá a estragar completamente tudo</td>\n",
       "      <td>[estrag, complet, tud, ]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            created_at sentiment  \\\n",
       "0  2021-07-04 22:17:21  Positivo   \n",
       "1  2019-11-30 14:35:33  Negativo   \n",
       "2  2022-01-11 12:12:28  Negativo   \n",
       "3  2019-11-19 14:52:06  Positivo   \n",
       "4  2021-12-28 23:09:10  Negativo   \n",
       "\n",
       "                                          normalized  \\\n",
       "0  após ficar em silêncio na cpi da covid carlos ...   \n",
       "1  para de lamber fone de ouvido lista de doenças...   \n",
       "2  definição de vacina é um tipo de substância ví...   \n",
       "3        por que o iphone é mais seguro contra vírus   \n",
       "4             covid tá a estragar completamente tudo   \n",
       "\n",
       "                         normalized_text_stemm_emoji  \n",
       "0  [após, fic, silênci, cpi, carl, wizard, retorn...  \n",
       "1  [lamb, fon, ouv, list, doenç, transmiss, oral,...  \n",
       "2  [defin, vacin, tip, subst, bactér, introduz, c...  \n",
       "3                            [iphon, segur, contr, ]  \n",
       "4                           [estrag, complet, tud, ]  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['created_at', 'sentiment', 'normalized', 'normalized_text_stemm_emoji']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1041154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(n=20000, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e4abc210",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df['sentiment'].replace(['Positivo','Negativo','Neutro'],[2,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b9e4e89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "labels = to_categorical(df['label'], num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e4a5108b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "01f7eeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['normalized_text_stemm_emoji'].apply(lambda x: ' '.join(x))\n",
    "# X = df['normalized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "82f73723",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X , labels, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "69fc6b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((16000,), (16000, 3), (4000,), (4000, 3))\n"
     ]
    }
   ],
   "source": [
    "print((X_train.shape, y_train.shape, X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9fd0fac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.models import Sequential\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c3e8905f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "emb_dim = 256\n",
    "batch_size = 256\n",
    "n_most_common_words = 2000\n",
    "max_len = 130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8f58f4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15878 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=n_most_common_words, lower=True)\n",
    "tokenizer.fit_on_texts(df['normalized_text_stemm_emoji'].values)\n",
    "sequences = tokenizer.texts_to_sequences(df['normalized_text_stemm_emoji'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "X = sequence.pad_sequences(sequences, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a1aaa320",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X , labels, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "176530cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 130, 256)          512000    \n",
      "                                                                 \n",
      " spatial_dropout1d_1 (Spatia  (None, 130, 256)         0         \n",
      " lDropout1D)                                                     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 32)                36992     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 549,091\n",
      "Trainable params: 549,091\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(n_most_common_words, emb_dim, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.1))\n",
    "model.add(LSTM(32, dropout=0.1, recurrent_dropout=0.1))\n",
    "# model.add(Dense(32, activation=\"relu\"))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b5716470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "50/50 [==============================] - 8s 137ms/step - loss: 0.9944 - acc: 0.5177 - val_loss: 0.9194 - val_acc: 0.5547\n",
      "Epoch 2/50\n",
      "50/50 [==============================] - 7s 134ms/step - loss: 0.7795 - acc: 0.6576 - val_loss: 0.7700 - val_acc: 0.6622\n",
      "Epoch 3/50\n",
      "50/50 [==============================] - 7s 135ms/step - loss: 0.6182 - acc: 0.7484 - val_loss: 0.7523 - val_acc: 0.6850\n",
      "Epoch 4/50\n",
      "50/50 [==============================] - 7s 135ms/step - loss: 0.5589 - acc: 0.7791 - val_loss: 0.7505 - val_acc: 0.6922\n",
      "Epoch 5/50\n",
      "50/50 [==============================] - 7s 134ms/step - loss: 0.5283 - acc: 0.7923 - val_loss: 0.7805 - val_acc: 0.6856\n",
      "Epoch 6/50\n",
      "50/50 [==============================] - 7s 134ms/step - loss: 0.5078 - acc: 0.8034 - val_loss: 0.7966 - val_acc: 0.6878\n",
      "Epoch 7/50\n",
      "50/50 [==============================] - 7s 136ms/step - loss: 0.4881 - acc: 0.8151 - val_loss: 0.8096 - val_acc: 0.6919\n",
      "Epoch 8/50\n",
      "50/50 [==============================] - 7s 135ms/step - loss: 0.4699 - acc: 0.8199 - val_loss: 0.8321 - val_acc: 0.6875\n",
      "Epoch 9/50\n",
      "50/50 [==============================] - 7s 136ms/step - loss: 0.4489 - acc: 0.8287 - val_loss: 0.8568 - val_acc: 0.6891\n",
      "Epoch 10/50\n",
      "50/50 [==============================] - 7s 137ms/step - loss: 0.4274 - acc: 0.8387 - val_loss: 0.9051 - val_acc: 0.6850\n",
      "Epoch 11/50\n",
      "50/50 [==============================] - 7s 136ms/step - loss: 0.4039 - acc: 0.8487 - val_loss: 0.9312 - val_acc: 0.6850\n",
      "Epoch 12/50\n",
      "50/50 [==============================] - 7s 137ms/step - loss: 0.3842 - acc: 0.8552 - val_loss: 0.9961 - val_acc: 0.6819\n",
      "Epoch 13/50\n",
      "50/50 [==============================] - 7s 133ms/step - loss: 0.3653 - acc: 0.8632 - val_loss: 1.0215 - val_acc: 0.6734\n",
      "Epoch 14/50\n",
      "50/50 [==============================] - 7s 134ms/step - loss: 0.3402 - acc: 0.8745 - val_loss: 1.0831 - val_acc: 0.6728\n",
      "Epoch 15/50\n",
      "50/50 [==============================] - 7s 134ms/step - loss: 0.3259 - acc: 0.8784 - val_loss: 1.1307 - val_acc: 0.6762\n",
      "Epoch 16/50\n",
      "50/50 [==============================] - 7s 134ms/step - loss: 0.3083 - acc: 0.8861 - val_loss: 1.1687 - val_acc: 0.6703\n",
      "Epoch 17/50\n",
      "50/50 [==============================] - 7s 134ms/step - loss: 0.2946 - acc: 0.8933 - val_loss: 1.2143 - val_acc: 0.6644\n",
      "Epoch 18/50\n",
      "50/50 [==============================] - 7s 134ms/step - loss: 0.2795 - acc: 0.8976 - val_loss: 1.2612 - val_acc: 0.6684\n",
      "Epoch 19/50\n",
      "50/50 [==============================] - 7s 134ms/step - loss: 0.2701 - acc: 0.9001 - val_loss: 1.3142 - val_acc: 0.6700\n",
      "Epoch 20/50\n",
      "50/50 [==============================] - 7s 134ms/step - loss: 0.2561 - acc: 0.9072 - val_loss: 1.3439 - val_acc: 0.6650\n",
      "Epoch 21/50\n",
      "50/50 [==============================] - 7s 135ms/step - loss: 0.2397 - acc: 0.9137 - val_loss: 1.4447 - val_acc: 0.6644\n",
      "Epoch 22/50\n",
      "50/50 [==============================] - 7s 134ms/step - loss: 0.2313 - acc: 0.9177 - val_loss: 1.4736 - val_acc: 0.6684\n",
      "Epoch 23/50\n",
      "50/50 [==============================] - 7s 135ms/step - loss: 0.2234 - acc: 0.9195 - val_loss: 1.5095 - val_acc: 0.6641\n",
      "Epoch 24/50\n",
      "50/50 [==============================] - 7s 135ms/step - loss: 0.2120 - acc: 0.9237 - val_loss: 1.4916 - val_acc: 0.6616\n",
      "Epoch 25/50\n",
      "50/50 [==============================] - 7s 135ms/step - loss: 0.2075 - acc: 0.9257 - val_loss: 1.5231 - val_acc: 0.6603\n",
      "Epoch 26/50\n",
      "50/50 [==============================] - 7s 135ms/step - loss: 0.2016 - acc: 0.9273 - val_loss: 1.5597 - val_acc: 0.6600\n",
      "Epoch 27/50\n",
      "50/50 [==============================] - 7s 135ms/step - loss: 0.1915 - acc: 0.9309 - val_loss: 1.6150 - val_acc: 0.6634\n",
      "Epoch 28/50\n",
      "50/50 [==============================] - 7s 135ms/step - loss: 0.1778 - acc: 0.9385 - val_loss: 1.6514 - val_acc: 0.6597\n",
      "Epoch 29/50\n",
      "50/50 [==============================] - 7s 135ms/step - loss: 0.1673 - acc: 0.9420 - val_loss: 1.7088 - val_acc: 0.6612\n",
      "Epoch 30/50\n",
      "50/50 [==============================] - 7s 135ms/step - loss: 0.1579 - acc: 0.9447 - val_loss: 1.7868 - val_acc: 0.6612\n",
      "Epoch 31/50\n",
      "50/50 [==============================] - 7s 134ms/step - loss: 0.1562 - acc: 0.9475 - val_loss: 1.8258 - val_acc: 0.6666\n",
      "Epoch 32/50\n",
      "50/50 [==============================] - 7s 134ms/step - loss: 0.1523 - acc: 0.9474 - val_loss: 1.8410 - val_acc: 0.6616\n",
      "Epoch 33/50\n",
      "50/50 [==============================] - 7s 134ms/step - loss: 0.1464 - acc: 0.9490 - val_loss: 1.8850 - val_acc: 0.6587\n",
      "Epoch 34/50\n",
      "50/50 [==============================] - 7s 134ms/step - loss: 0.1373 - acc: 0.9533 - val_loss: 1.8664 - val_acc: 0.6625\n",
      "Epoch 35/50\n",
      "50/50 [==============================] - 7s 134ms/step - loss: 0.1308 - acc: 0.9555 - val_loss: 1.9615 - val_acc: 0.6609\n",
      "Epoch 36/50\n",
      "50/50 [==============================] - 7s 134ms/step - loss: 0.1250 - acc: 0.9566 - val_loss: 1.9358 - val_acc: 0.6594\n",
      "Epoch 37/50\n",
      "50/50 [==============================] - 7s 134ms/step - loss: 0.1232 - acc: 0.9587 - val_loss: 2.0039 - val_acc: 0.6584\n",
      "Epoch 38/50\n",
      "50/50 [==============================] - 7s 134ms/step - loss: 0.1175 - acc: 0.9603 - val_loss: 2.0192 - val_acc: 0.6631\n",
      "Epoch 39/50\n",
      "50/50 [==============================] - 7s 135ms/step - loss: 0.1103 - acc: 0.9628 - val_loss: 2.0834 - val_acc: 0.6619\n",
      "Epoch 40/50\n",
      "50/50 [==============================] - 7s 135ms/step - loss: 0.1064 - acc: 0.9641 - val_loss: 2.1661 - val_acc: 0.6625\n",
      "Epoch 41/50\n",
      "50/50 [==============================] - 7s 135ms/step - loss: 0.1256 - acc: 0.9578 - val_loss: 2.0512 - val_acc: 0.6600\n",
      "Epoch 42/50\n",
      "50/50 [==============================] - 7s 135ms/step - loss: 0.1039 - acc: 0.9648 - val_loss: 2.1080 - val_acc: 0.6581\n",
      "Epoch 43/50\n",
      "50/50 [==============================] - 7s 135ms/step - loss: 0.0947 - acc: 0.9690 - val_loss: 2.1469 - val_acc: 0.6597\n",
      "Epoch 44/50\n",
      "50/50 [==============================] - 7s 137ms/step - loss: 0.0896 - acc: 0.9707 - val_loss: 2.1703 - val_acc: 0.6597\n",
      "Epoch 45/50\n",
      "50/50 [==============================] - 7s 135ms/step - loss: 0.0861 - acc: 0.9714 - val_loss: 2.2121 - val_acc: 0.6578\n",
      "Epoch 46/50\n",
      "50/50 [==============================] - 7s 134ms/step - loss: 0.0898 - acc: 0.9702 - val_loss: 2.2929 - val_acc: 0.6612\n",
      "Epoch 47/50\n",
      "50/50 [==============================] - 7s 134ms/step - loss: 0.0807 - acc: 0.9741 - val_loss: 2.2972 - val_acc: 0.6531\n",
      "Epoch 48/50\n",
      "50/50 [==============================] - 7s 134ms/step - loss: 0.0822 - acc: 0.9724 - val_loss: 2.3170 - val_acc: 0.6600\n",
      "Epoch 49/50\n",
      "50/50 [==============================] - 7s 134ms/step - loss: 0.0822 - acc: 0.9717 - val_loss: 2.3210 - val_acc: 0.6578\n",
      "Epoch 50/50\n",
      "50/50 [==============================] - 7s 135ms/step - loss: 0.0773 - acc: 0.9748 - val_loss: 2.3009 - val_acc: 0.6637\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f86cd788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 1s 6ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.73      0.72      1930\n",
      "           1       0.63      0.61      0.62      1191\n",
      "           2       0.60      0.59      0.59       879\n",
      "\n",
      "    accuracy                           0.66      4000\n",
      "   macro avg       0.65      0.64      0.64      4000\n",
      "weighted avg       0.66      0.66      0.66      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "preds = np.argmax(model.predict(X_test), axis = 1)\n",
    "print(classification_report(np.argmax(y_test, axis = 1), preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54431e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
