{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7915485d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.7/site-packages (23.2.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c9626ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/site-packages (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.7/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.7/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.7/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.7/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (57.5.0)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.41.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in /usr/local/lib/python3.7/site-packages (4.30.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.7/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/site-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/site-packages (from transformers) (5.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.7/site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/site-packages (from transformers) (6.7.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (0.6.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.7/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests->transformers) (2023.7.22)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: emoji in /usr/local/lib/python3.7/site-packages (2.8.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bca4d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43b1286b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-07-04 22:17:21</td>\n",
       "      <td>RT @emirsader: Ap√≥s ficar em sil√™ncio na CPI d...</td>\n",
       "      <td>Positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-11-30 14:35:33</td>\n",
       "      <td>RT @VittorGuidoni: para de lamber fone de ouvi...</td>\n",
       "      <td>Negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-11 12:12:28</td>\n",
       "      <td>@exposed_exposer @CarlaZambelli38 @andrizek De...</td>\n",
       "      <td>Negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-11-19 14:52:06</td>\n",
       "      <td>RT @canaltech: Por que o iPhone √© mais seguro ...</td>\n",
       "      <td>Positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-12-28 23:09:10</td>\n",
       "      <td>RT @mariareinhardtt: covid t√° a estragar compl...</td>\n",
       "      <td>Negativo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            created_at                                               text  \\\n",
       "0  2021-07-04 22:17:21  RT @emirsader: Ap√≥s ficar em sil√™ncio na CPI d...   \n",
       "1  2019-11-30 14:35:33  RT @VittorGuidoni: para de lamber fone de ouvi...   \n",
       "2  2022-01-11 12:12:28  @exposed_exposer @CarlaZambelli38 @andrizek De...   \n",
       "3  2019-11-19 14:52:06  RT @canaltech: Por que o iPhone √© mais seguro ...   \n",
       "4  2021-12-28 23:09:10  RT @mariareinhardtt: covid t√° a estragar compl...   \n",
       "\n",
       "  sentiment  \n",
       "0  Positivo  \n",
       "1  Negativo  \n",
       "2  Negativo  \n",
       "3  Positivo  \n",
       "4  Negativo  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data_pt_brazil_normalized_emoji_merged.csv')\n",
    "df = df[['created_at', 'text', 'sentiment']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b253e66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_bank = {\n",
    "  \",:(\": \"üòì\",\n",
    "  \",:)\": \"üòÖ\",\n",
    "  \",:-(\": \"üòì\",\n",
    "  \",:-)\": \"üòÖ\",\n",
    "  \"0:)\": \"üòá\",\n",
    "  \"0:-)\": \"üòá\",\n",
    "  \"8-)\": \"üòé\",\n",
    "  \":$\": \"üòí\",\n",
    "  \":'(\": \"üò¢\",\n",
    "  \":')\": \"üòÇ\",\n",
    "  \":'-(\": \"üò¢\",\n",
    "  \":'-)\": \"üòÇ\",\n",
    "  \":'-D\": \"üòÇ\",\n",
    "  \":'D\": \"üòÇ\",\n",
    "  \":(\": \"üò¶\",\n",
    "  \":)\": \"üòÉ\",\n",
    "  \":*\": \"üòó\",\n",
    "  \":,'(\": \"üò≠\",\n",
    "  \":,'-(\": \"üò≠\",\n",
    "  \":,(\": \"üò¢\",\n",
    "  \":,)\": \"üòÇ\",\n",
    "  \":,-(\": \"üò¢\",\n",
    "  \":,-)\": \"üòÇ\",\n",
    "  \":,-D\": \"üòÇ\",\n",
    "  \":,D\": \"üòÇ\",\n",
    "  \":-$\": \"üòí\",\n",
    "  \":-(\": \"üò¶\",\n",
    "  \":-)\": \"üòÉ\",\n",
    "  \":-*\": \"üòó\",\n",
    "  \":-/\": \"üòï\",\n",
    "  \":-@\": \"üò°\",\n",
    "  \":-D\": \"üòÑ\",\n",
    "  \":-o\": \"üòÆ\",\n",
    "  \":-O\": \"üòÆ\",\n",
    "  \":-P\": \"üòõ\",\n",
    "  \":-S\": \"üòí\",\n",
    "  \":-Z\": \"üòí\",\n",
    "  \":-|\": \"üòê\",\n",
    "  \":/\": \"üòï\",\n",
    "  \":@\": \"üò°\",\n",
    "  \":D\": \"üòÑ\",\n",
    "  \":o\": \"üòÆ\",\n",
    "  \":O\": \"üòÆ\",\n",
    "  \":P\": \"üòõ\",\n",
    "  \":s\": \"üòí\",\n",
    "  \":z\": \"üòí\",\n",
    "  \":|\": \"üòê\",\n",
    "  \";(\": \"üò≠\",\n",
    "  \";)\": \"üòâ\",\n",
    "  \";-(\": \"üò≠\",\n",
    "  \";-)\": \"üòâ\",\n",
    "  \"]:)\": \"üòà\",\n",
    "  \"]:-)\": \"üòà\",\n",
    "  \"B-)\": \"üòé\",\n",
    "  \"o:)\": \"üòá\",\n",
    "  \"O:)\": \"üòá\",\n",
    "  \"O:-)\": \"üòá\",\n",
    "  \"o:-)\": \"üòá\",\n",
    "  \"X-)\": \"üòÜ\",\n",
    "  \"x-)\": \"üòÜ\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26621668",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = ['rt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bcfd786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import emoji\n",
    "\n",
    "def extract_emojis(sample):\n",
    "    return ' '.join(c for c in sample if c in emoji.EMOJI_DATA).split()\n",
    "\n",
    "def remove_user_from_text(words):\n",
    "    return \" \".join(filter(lambda x:x[0]!='@', words.split()))\n",
    "\n",
    "def remove_double_space(sample):\n",
    "    return \" \".join(sample.split())\n",
    "\n",
    "def remove_URL(sample):\n",
    "    \"\"\"Remove URLs from a sample string\"\"\"\n",
    "    return re.sub(r\"http\\S+\", \"\", sample)\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    return list(map(lambda x: x.lower(), words))\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def clean_text(sample):\n",
    "    emoji_pat = '[\\U0001F300-\\U0001F64F\\U0001F680-\\U0001F6FF\\u2600-\\u26FF\\u2700-\\u27BF]'\n",
    "    shrink_whitespace_reg = re.compile(r'\\s{2,}')\n",
    "    reg = re.compile(r'({})|[^a-zA-Z]'.format(emoji_pat)) # line a\n",
    "    result = reg.sub(lambda x: ' {} '.format(x.group(1)) if x.group(1) else ' ', sample)\n",
    "    return shrink_whitespace_reg.sub(' ', result)\n",
    "\n",
    "def replace_asci_emoji(sample):\n",
    "    address = sample\n",
    "    for k,v in emoji_bank.items():\n",
    "        address = address.replace(k, v)\n",
    "    return address\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords_list:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_user_from_text(words)\n",
    "    words = to_lowercase(words.split())\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "def preprocess(sample):\n",
    "    sample = remove_URL(sample)\n",
    "    sample = replace_asci_emoji(sample)\n",
    "#     sample = clean_text(sample)\n",
    "    sample = remove_double_space(sample)\n",
    "    return normalize(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "379b391a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-07-04 22:17:21</td>\n",
       "      <td>RT @emirsader: Ap√≥s ficar em sil√™ncio na CPI d...</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>ap√≥s ficar em sil√™ncio na cpi da covid carlos ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-11-30 14:35:33</td>\n",
       "      <td>RT @VittorGuidoni: para de lamber fone de ouvi...</td>\n",
       "      <td>Negativo</td>\n",
       "      <td>para de lamber fone de ouvido lista de doen√ßas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-11 12:12:28</td>\n",
       "      <td>@exposed_exposer @CarlaZambelli38 @andrizek De...</td>\n",
       "      <td>Negativo</td>\n",
       "      <td>defini√ß√£o de vacina √© um tipo de subst√¢ncia v√≠...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-11-19 14:52:06</td>\n",
       "      <td>RT @canaltech: Por que o iPhone √© mais seguro ...</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>por que o iphone √© mais seguro contra v√≠rus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-12-28 23:09:10</td>\n",
       "      <td>RT @mariareinhardtt: covid t√° a estragar compl...</td>\n",
       "      <td>Negativo</td>\n",
       "      <td>covid t√° a estragar completamente tudo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            created_at                                               text  \\\n",
       "0  2021-07-04 22:17:21  RT @emirsader: Ap√≥s ficar em sil√™ncio na CPI d...   \n",
       "1  2019-11-30 14:35:33  RT @VittorGuidoni: para de lamber fone de ouvi...   \n",
       "2  2022-01-11 12:12:28  @exposed_exposer @CarlaZambelli38 @andrizek De...   \n",
       "3  2019-11-19 14:52:06  RT @canaltech: Por que o iPhone √© mais seguro ...   \n",
       "4  2021-12-28 23:09:10  RT @mariareinhardtt: covid t√° a estragar compl...   \n",
       "\n",
       "  sentiment                                         normalized  \n",
       "0  Positivo  ap√≥s ficar em sil√™ncio na cpi da covid carlos ...  \n",
       "1  Negativo  para de lamber fone de ouvido lista de doen√ßas...  \n",
       "2  Negativo  defini√ß√£o de vacina √© um tipo de subst√¢ncia v√≠...  \n",
       "3  Positivo        por que o iphone √© mais seguro contra v√≠rus  \n",
       "4  Negativo             covid t√° a estragar completamente tudo  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['normalized'] = df.apply(lambda x: \" \".join(preprocess(x['text'])), axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9a926b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-07-04 22:17:21</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>ap√≥s ficar em sil√™ncio na cpi da covid carlos ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-11-30 14:35:33</td>\n",
       "      <td>Negativo</td>\n",
       "      <td>para de lamber fone de ouvido lista de doen√ßas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-11 12:12:28</td>\n",
       "      <td>Negativo</td>\n",
       "      <td>defini√ß√£o de vacina √© um tipo de subst√¢ncia v√≠...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-11-19 14:52:06</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>por que o iphone √© mais seguro contra v√≠rus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-12-28 23:09:10</td>\n",
       "      <td>Negativo</td>\n",
       "      <td>covid t√° a estragar completamente tudo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            created_at sentiment  \\\n",
       "0  2021-07-04 22:17:21  Positivo   \n",
       "1  2019-11-30 14:35:33  Negativo   \n",
       "2  2022-01-11 12:12:28  Negativo   \n",
       "3  2019-11-19 14:52:06  Positivo   \n",
       "4  2021-12-28 23:09:10  Negativo   \n",
       "\n",
       "                                          normalized  \n",
       "0  ap√≥s ficar em sil√™ncio na cpi da covid carlos ...  \n",
       "1  para de lamber fone de ouvido lista de doen√ßas...  \n",
       "2  defini√ß√£o de vacina √© um tipo de subst√¢ncia v√≠...  \n",
       "3        por que o iphone √© mais seguro contra v√≠rus  \n",
       "4             covid t√° a estragar completamente tudo  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['created_at', 'sentiment', 'normalized']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba5e612d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df['sentiment'].replace(['Positivo','Negativo','Neutro'],[2,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83968e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.467800\n",
       "1    0.294875\n",
       "2    0.237325\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea1030fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(n=20000, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89912b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train dataset into train, validation and test sets\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df['normalized'], df['sentiment'], \n",
    "                                                                    random_state=42, \n",
    "                                                                    test_size=0.3, \n",
    "                                                                    stratify=df['sentiment'])\n",
    "\n",
    "\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=42, \n",
    "                                                                test_size=0.5, \n",
    "                                                                stratify=temp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1c1cc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "bert = AutoModel.from_pretrained('neuralmind/bert-base-portuguese-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "831bd73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73b64c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2383: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa95c8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train set\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "# for validation set\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "# for test set\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e6def24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 32\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1811c329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d29a3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "      \n",
    "        super(BERT_Arch, self).__init__()\n",
    "\n",
    "        self.bert = bert \n",
    "      \n",
    "      # dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "      \n",
    "      # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "\n",
    "      # dense layer 1\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "      \n",
    "      # dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512,3)\n",
    "\n",
    "      #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "      #pass the inputs to the model  \n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask,return_dict=False)\n",
    "      \n",
    "        x = self.fc1(cls_hs)\n",
    "\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "      # output layer\n",
    "        x = self.fc2(x)\n",
    "      \n",
    "      # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db1f779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available = lambda : False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4f9fc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)\n",
    "\n",
    "# push the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8412a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/transformers/optimization.py:415: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# optimizer from hugging face transformers\n",
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d022c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7019655  1.13793384 1.43545576]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/utils/validation.py:72: FutureWarning: Pass classes=[0 1 2], y=9386     0\n",
      "3665     0\n",
      "1018     0\n",
      "15040    1\n",
      "5089     1\n",
      "        ..\n",
      "12482    1\n",
      "1110     2\n",
      "7877     0\n",
      "15786    0\n",
      "15623    1\n",
      "Name: sentiment, Length: 14000, dtype: int64 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#compute the class weights\n",
    "class_wts = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n",
    "\n",
    "print(class_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62735ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class weights to tensor\n",
    "weights= torch.tensor(class_wts,dtype=torch.float)\n",
    "weights = weights.to(device)\n",
    "\n",
    "# loss function\n",
    "cross_entropy  = nn.NLLLoss(weight=weights) \n",
    "\n",
    "# number of training epochs\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f36dcec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "  \n",
    "    model.train()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "  \n",
    "  # iterate over batches\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "    \n",
    "    # progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "    # push the batch to gpu\n",
    "        batch = [r.to(device) for r in batch]\n",
    " \n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "    # clear previously calculated gradients \n",
    "        model.zero_grad()        \n",
    "\n",
    "    # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "\n",
    "    # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "\n",
    "    # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "    # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "\n",
    "    # append the model predictions\n",
    "        total_preds.append(preds)\n",
    "\n",
    "  # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "  \n",
    "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "  #returns the loss and predictions\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5a031c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "  \n",
    "    print(\"\\nEvaluating...\")\n",
    "  \n",
    "  # deactivate dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "\n",
    "  # iterate over batches\n",
    "    for step,batch in enumerate(val_dataloader):\n",
    "    \n",
    "    # Progress update every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            \n",
    "      # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "    # push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "    # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "      \n",
    "      # model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "\n",
    "      # compute the validation loss between actual and predicted values\n",
    "            loss = cross_entropy(preds,labels)\n",
    "\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "\n",
    "            total_preds.append(preds)\n",
    "\n",
    "  # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de3c960c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 10\n",
      "  Batch    50  of    438.\n",
      "  Batch   100  of    438.\n",
      "  Batch   150  of    438.\n",
      "  Batch   200  of    438.\n",
      "  Batch   250  of    438.\n",
      "  Batch   300  of    438.\n",
      "  Batch   350  of    438.\n",
      "  Batch   400  of    438.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     94.\n",
      "\n",
      "Training Loss: 1.044\n",
      "Validation Loss: 1.035\n",
      "\n",
      " Epoch 2 / 10\n",
      "  Batch    50  of    438.\n",
      "  Batch   100  of    438.\n",
      "  Batch   150  of    438.\n",
      "  Batch   200  of    438.\n",
      "  Batch   250  of    438.\n",
      "  Batch   300  of    438.\n",
      "  Batch   350  of    438.\n",
      "  Batch   400  of    438.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     94.\n",
      "\n",
      "Training Loss: 1.012\n",
      "Validation Loss: 0.990\n",
      "\n",
      " Epoch 3 / 10\n",
      "  Batch    50  of    438.\n",
      "  Batch   100  of    438.\n",
      "  Batch   150  of    438.\n",
      "  Batch   200  of    438.\n",
      "  Batch   250  of    438.\n",
      "  Batch   300  of    438.\n",
      "  Batch   350  of    438.\n",
      "  Batch   400  of    438.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     94.\n",
      "\n",
      "Training Loss: 0.996\n",
      "Validation Loss: 0.995\n",
      "\n",
      " Epoch 4 / 10\n",
      "  Batch    50  of    438.\n",
      "  Batch   100  of    438.\n",
      "  Batch   150  of    438.\n",
      "  Batch   200  of    438.\n",
      "  Batch   250  of    438.\n",
      "  Batch   300  of    438.\n",
      "  Batch   350  of    438.\n",
      "  Batch   400  of    438.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     94.\n",
      "\n",
      "Training Loss: 0.990\n",
      "Validation Loss: 0.984\n",
      "\n",
      " Epoch 5 / 10\n",
      "  Batch    50  of    438.\n",
      "  Batch   100  of    438.\n",
      "  Batch   150  of    438.\n",
      "  Batch   200  of    438.\n",
      "  Batch   250  of    438.\n",
      "  Batch   300  of    438.\n",
      "  Batch   350  of    438.\n",
      "  Batch   400  of    438.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     94.\n",
      "\n",
      "Training Loss: 0.978\n",
      "Validation Loss: 0.968\n",
      "\n",
      " Epoch 6 / 10\n",
      "  Batch    50  of    438.\n",
      "  Batch   100  of    438.\n",
      "  Batch   150  of    438.\n",
      "  Batch   200  of    438.\n",
      "  Batch   250  of    438.\n",
      "  Batch   300  of    438.\n",
      "  Batch   350  of    438.\n",
      "  Batch   400  of    438.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     94.\n",
      "\n",
      "Training Loss: 0.976\n",
      "Validation Loss: 0.972\n",
      "\n",
      " Epoch 7 / 10\n",
      "  Batch    50  of    438.\n",
      "  Batch   100  of    438.\n",
      "  Batch   150  of    438.\n",
      "  Batch   200  of    438.\n",
      "  Batch   250  of    438.\n",
      "  Batch   300  of    438.\n",
      "  Batch   350  of    438.\n",
      "  Batch   400  of    438.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     94.\n",
      "\n",
      "Training Loss: 0.972\n",
      "Validation Loss: 0.956\n",
      "\n",
      " Epoch 8 / 10\n",
      "  Batch    50  of    438.\n",
      "  Batch   100  of    438.\n",
      "  Batch   150  of    438.\n",
      "  Batch   200  of    438.\n",
      "  Batch   250  of    438.\n",
      "  Batch   300  of    438.\n",
      "  Batch   350  of    438.\n",
      "  Batch   400  of    438.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     94.\n",
      "\n",
      "Training Loss: 0.965\n",
      "Validation Loss: 0.978\n",
      "\n",
      " Epoch 9 / 10\n",
      "  Batch    50  of    438.\n",
      "  Batch   100  of    438.\n",
      "  Batch   150  of    438.\n",
      "  Batch   200  of    438.\n",
      "  Batch   250  of    438.\n",
      "  Batch   300  of    438.\n",
      "  Batch   350  of    438.\n",
      "  Batch   400  of    438.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     94.\n",
      "\n",
      "Training Loss: 0.960\n",
      "Validation Loss: 0.960\n",
      "\n",
      " Epoch 10 / 10\n",
      "  Batch    50  of    438.\n",
      "  Batch   100  of    438.\n",
      "  Batch   150  of    438.\n",
      "  Batch   200  of    438.\n",
      "  Batch   250  of    438.\n",
      "  Batch   300  of    438.\n",
      "  Batch   350  of    438.\n",
      "  Batch   400  of    438.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     94.\n",
      "\n",
      "Training Loss: 0.956\n",
      "Validation Loss: 0.953\n"
     ]
    }
   ],
   "source": [
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01d5dc32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load weights of best model\n",
    "path = 'saved_weights.pt'\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ca5c9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a0605e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for test data\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq[0:3000].to(device), test_mask[0:3000].to(device))\n",
    "    preds = preds.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3b3fa3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.68      0.66      1425\n",
      "           1       0.57      0.49      0.53       879\n",
      "           2       0.41      0.44      0.43       696\n",
      "\n",
      "    accuracy                           0.57      3000\n",
      "   macro avg       0.54      0.54      0.54      3000\n",
      "weighted avg       0.57      0.57      0.57      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model's performance\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y[0:3000], preds[0:3000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6def3dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(preds, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3ff72250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 0, ..., 0, 1, 2])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b366f6e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
